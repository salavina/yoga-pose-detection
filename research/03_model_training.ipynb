{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/saboa/projects/yoga-pose-detection/research'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/saboa/projects/yoga-pose-detection'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    root_dir: Path\n",
    "    resnet_trained_model_path: Path\n",
    "    resnet_updated_base_model_path: Path\n",
    "    training_data: Path\n",
    "    mlflow_uri: str\n",
    "    all_params: dict\n",
    "    params_augmentation: bool\n",
    "    params_image_size: list\n",
    "    params_batch_size: int\n",
    "    params_epochs: int\n",
    "    params_learning_rate: float\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ypd.constants import *\n",
    "from ypd.utils.common import read_yaml, create_directories\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "MLFLOW_TRACKING_URI = os.environ[\"MLFLOW_TRACKING_URI\"]\n",
    "MLFLOW_TRACKING_USERNAME = os.environ[\"MLFLOW_TRACKING_USERNAME\"]\n",
    "MLFLOW_TRACKING_PASSWORD = os.environ[\"MLFLOW_TRACKING_PASSWORD\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class configurationManager:\n",
    "    def __init__(self,\n",
    "                 config_file_path=CONFIG_FILE_PATH,\n",
    "                 params_file_path=PARAMS_FILE_PATH):\n",
    "        \n",
    "        self.config = read_yaml(config_file_path)\n",
    "        self.params = read_yaml(params_file_path)\n",
    "        \n",
    "        create_directories([self.config.artifacts_root])\n",
    "    \n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        model_training = self.config.model_training\n",
    "        prepare_base_model = self.config.prepare_base_model\n",
    "        training_data = os.path.join(self.config.data_ingestion.root_dir, 'yoga-poses-dataset')\n",
    "        \n",
    "        create_directories([model_training.root_dir])\n",
    "        \n",
    "        training_config = TrainingConfig(\n",
    "            root_dir= model_training.root_dir,\n",
    "            resnet_trained_model_path= model_training.resnet_trained_model_path,\n",
    "            resnet_updated_base_model_path= prepare_base_model.resnet_updated_base_model_path,\n",
    "            training_data= training_data,\n",
    "            mlflow_uri = MLFLOW_TRACKING_URI,\n",
    "            all_params = self.params,\n",
    "            params_augmentation= self.params.AUGMENTATION,\n",
    "            params_image_size= self.params.IMAGE_SIZE,\n",
    "            params_batch_size= self.params.BATCH_SIZE,\n",
    "            params_epochs= self.params.EPOCHS,\n",
    "            params_learning_rate= self.params.LEARNING_RATE\n",
    "        )\n",
    "        \n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, \\\n",
    "Resize, CenterCrop\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from urllib.parse import urlparse\n",
    "from PIL import ImageFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer(object):\n",
    "    def __init__(self, config:TrainingConfig, loss_fn=None, optimizer=None):\n",
    "        self.config = config\n",
    "        self.model = self.load_model()\n",
    "        self.loss_fn = loss_fn if loss_fn else nn.CrossEntropyLoss(reduction='mean')\n",
    "        self.optimizer = optimizer if optimizer else optim.Adam(self.model.parameters(), lr=self.config.params_learning_rate)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.train_loader, self.val_loader = self.set_loaders()\n",
    "        \n",
    "        self.losses = []\n",
    "        self.val_losses = []\n",
    "        self.accuracy = []\n",
    "        self.val_accuracy = []\n",
    "        self.total_epoches = 0\n",
    "        \n",
    "        self.train_step_fn = self._make_train_step_fn()\n",
    "        self.val_step_fn = self._make_val_step_fn()\n",
    "        \n",
    "    def load_model(self):\n",
    "        return torch.load(self.config.resnet_updated_base_model_path)\n",
    "    \n",
    "    def set_seed(self, seed=42):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.manual_seed(seed)\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    def set_loaders(self):\n",
    "        # image net statistics\n",
    "        ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "        normalizer = Normalize(mean=[0.485, 0.456, 0.406], std=[0.229,0.224,0.225])\n",
    "        \n",
    "        composer = Compose([Resize(256), CenterCrop(224), ToTensor(), normalizer])\n",
    "        \n",
    "        train_data = ImageFolder(root=os.path.join(self.config.training_data,'DATASET/TRAIN'), transform=composer)\n",
    "        val_data = ImageFolder(root=os.path.join(self.config.training_data,'DATASET/TEST'), transform=composer)\n",
    "        \n",
    "        train_loader = DataLoader(train_data, batch_size=self.config.params_batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_data, batch_size=self.config.params_batch_size)\n",
    "        \n",
    "        return train_loader, val_loader\n",
    "    \n",
    "    # higher order function to be set and built globally and constructed the inner fuction without knowning x and y before hand\n",
    "    def _make_train_step_fn(self):\n",
    "        # single batch operation\n",
    "        def perform_train_step_fn(x,y):\n",
    "            # set the train mode\n",
    "            self.model.train()\n",
    "            \n",
    "            # step 1: compute model output\n",
    "            yhat = self.model(x)\n",
    "            \n",
    "            # step 2: compute the loss  \n",
    "            loss= self.loss_fn(yhat, y)\n",
    "            \n",
    "            # step 2': compute accuracy \n",
    "            yhat = torch.argmax(yhat,1)\n",
    "            total_correct = (yhat ==y).sum().item()\n",
    "            total = y.shape[0]\n",
    "            acc = total_correct/total\n",
    "            \n",
    "            # step 3: compute the gradient\n",
    "            loss.backward()\n",
    "            \n",
    "            #step4: update parameters\n",
    "            self.optimizer.step()\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            #step 5: return the loss\n",
    "            return loss.item() , acc\n",
    "        return perform_train_step_fn\n",
    "    \n",
    "    def _make_val_step_fn(self):\n",
    "        # single batch operation\n",
    "        def perform_val_step_fn(x,y):\n",
    "            # set the model in val mode\n",
    "            self.model.eval()\n",
    "            \n",
    "            #step 1: compute the prediction\n",
    "            yhat = self.model(x)\n",
    "            \n",
    "            #step 2: compute the loss\n",
    "            loss = self.loss_fn(yhat, y)\n",
    "            # step 2': compute accuracy \n",
    "            yhat = torch.argmax(yhat,1)\n",
    "            total_correct = (yhat ==y).sum().item()\n",
    "            total = y.shape[0]\n",
    "            acc = total_correct/total\n",
    "            \n",
    "            return loss.item(), acc\n",
    "        return perform_val_step_fn\n",
    "    \n",
    "    def _mini_batch(self, validation=False):\n",
    "        # one epoch operation \n",
    "        if validation:\n",
    "            data_loader = self.val_loader\n",
    "            step_fn = self.val_step_fn\n",
    "            \n",
    "        else: \n",
    "            data_loader = self.train_loader\n",
    "            step_fn = self.train_step_fn\n",
    "            \n",
    "        if data_loader is None:\n",
    "            return None\n",
    "        \n",
    "        mini_batch_losses = []\n",
    "        mini_batch_accs = []\n",
    "        for x_batch, y_batch in data_loader:\n",
    "            x_batch = x_batch.to(self.device)\n",
    "            y_batch = y_batch.to(self.device)\n",
    "            \n",
    "            mini_batch_loss, mini_batch_acc = step_fn(x_batch,y_batch)\n",
    "            \n",
    "            mini_batch_losses.append(mini_batch_loss)\n",
    "            mini_batch_accs.append(mini_batch_acc)\n",
    "        \n",
    "        loss = np.mean(mini_batch_losses)\n",
    "        acc = np.mean(mini_batch_accs)\n",
    "        return loss, acc\n",
    "    \n",
    "    def train(self, seed=42):\n",
    "        self.set_seed(seed)\n",
    "        \n",
    "        for epoch in range(self.config.params_epochs):\n",
    "            self.total_epoches +=1\n",
    "            \n",
    "            # perform training on mini batches within 1 epoch\n",
    "            loss, acc = self._mini_batch(validation=False)\n",
    "            self.losses.append(loss)\n",
    "            self.accuracy.append(acc)\n",
    "            # now calc validation\n",
    "            with torch.no_grad():\n",
    "                val_loss, val_acc = self._mini_batch(validation=True)\n",
    "                self.val_losses.append(val_loss)\n",
    "                self.val_accuracy.append(val_acc)\n",
    "                \n",
    "            print(\n",
    "                f'\\nEpoch: {epoch+1} \\tTraining Loss: {loss:.4f} \\tValidation Loss: {val_loss:.4f}'\n",
    "            )\n",
    "            print(\n",
    "                f'\\t\\tTraining Accuracy: {100 * acc:.2f}%\\t Validation Accuracy: {100 * val_acc:.2f}%'\n",
    "            )\n",
    "        self.save_checkpoint()\n",
    "            \n",
    "    def save_checkpoint(self):\n",
    "        checkpoint = {'epoch': self.total_epoches,\n",
    "                      'model_state_dict': self.model.state_dict(),\n",
    "                      'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                      'loss': self.losses,\n",
    "                      'accuracy': self.accuracy,\n",
    "                      'val_loss': self.val_losses,\n",
    "                      'val_accuracy': self.val_accuracy\n",
    "                      }\n",
    "        torch.save(checkpoint, self.config.resnet_trained_model_path)\n",
    "        \n",
    "    def load_checkpoint(self):\n",
    "        checkpoint = torch.load(self.config.resnet_trained_model_path)\n",
    "        self.model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "        self.total_epoches = checkpoint[\"epoch\"]\n",
    "        self.losses = checkpoint[\"loss\"]\n",
    "        self.accuracy = checkpoint['accuracy']\n",
    "        self.val_accuracy = checkpoint['val_accuracy']\n",
    "        self.val_losses = checkpoint[\"val_loss\"]\n",
    "        self.model.train() # always use train for resuming traning\n",
    "        \n",
    "    def predict(self,x):\n",
    "        \n",
    "        self.model.eval()\n",
    "        \n",
    "        x_tensor = torch.as_tensor(x).float()\n",
    "        y_hat_tensor = self.model(x_tensor.to(self.device))\n",
    "        \n",
    "        # set it back to the train mode\n",
    "        self.model.train()\n",
    "        \n",
    "        return y_hat_tensor.detach().cpu().numpy()\n",
    "    \n",
    "    def log_into_mlflow(self):\n",
    "        mlflow.set_registry_uri(self.config.mlflow_uri)\n",
    "        tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "        \n",
    "        with mlflow.start_run():\n",
    "            mlflow.log_params(self.config.all_params)\n",
    "            mlflow.log_metrics({'train_loss': self.losses,'val_loss': self.val_losses, 'train_accuracy': self.accuracy, 'val_accuracy': self.val_accuracy})\n",
    "        \n",
    "            # Model registry does not work with file store\n",
    "            if tracking_url_type_store != \"file\":\n",
    "\n",
    "                # Register the model\n",
    "                # There are other ways to use the Model Registry, which depends on the use case,\n",
    "                # please refer to the doc for more information:\n",
    "                # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "                mlflow.pytorch.log_model(self.model, \"model\", registered_model_name=\"ResNet18Model\")\n",
    "            else:\n",
    "                mlflow.pytorch.log_model(self.model, \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-03-05 01:58:31,772: INFO: common: yaml file: config/config.yaml loaded successfully]\n",
      "[2024-03-05 01:58:31,774: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2024-03-05 01:58:31,775: INFO: common: created directory at: artifacts]\n",
      "[2024-03-05 01:58:31,775: INFO: common: created directory at: artifacts/training]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/saboa/projects/yoga-pose-detection/yoga/lib/python3.8/site-packages/PIL/Image.py:981: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0 \tTraining Loss: 6.7010 \tValidation Loss: 1.3267\n",
      "\t\tTraining Accuracy: 58.15%\t Validation Accuracy: 82.01%\n",
      "\n",
      "Epoch: 1 \tTraining Loss: 2.2466 \tValidation Loss: 0.8773\n",
      "\t\tTraining Accuracy: 76.03%\t Validation Accuracy: 91.60%\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ModelTrainer' object has no attribute 'all_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m     training\u001b[38;5;241m.\u001b[39mlog_into_mlflow()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[35], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m     training \u001b[38;5;241m=\u001b[39m ModelTrainer(config\u001b[38;5;241m=\u001b[39mtraining_config)\n\u001b[1;32m      5\u001b[0m     training\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m----> 6\u001b[0m     \u001b[43mtraining\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_into_mlflow\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "Cell \u001b[0;32mIn[32], line 186\u001b[0m, in \u001b[0;36mModelTrainer.log_into_mlflow\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    183\u001b[0m tracking_url_type_store \u001b[38;5;241m=\u001b[39m urlparse(mlflow\u001b[38;5;241m.\u001b[39mget_tracking_uri())\u001b[38;5;241m.\u001b[39mscheme\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m mlflow\u001b[38;5;241m.\u001b[39mstart_run():\n\u001b[0;32m--> 186\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_params(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_params\u001b[49m)\n\u001b[1;32m    187\u001b[0m     mlflow\u001b[38;5;241m.\u001b[39mlog_metrics({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_losses, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccuracy, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_accuracy})\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;66;03m# Model registry does not work with file store\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ModelTrainer' object has no attribute 'all_params'"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = configurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = ModelTrainer(config=training_config)\n",
    "    training.train()\n",
    "    training.log_into_mlflow()\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yoga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
